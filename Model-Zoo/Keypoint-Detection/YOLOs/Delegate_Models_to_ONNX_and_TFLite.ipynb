{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Load the Configuration of YOLOs\n",
        "\n",
        "<img src=\"https://colab.research.google.com/img/colab_favicon_256px.png\" alt=\"Running on Colab\" width='60'> [Runing on colab]()\n",
        "\n",
        "**1. Import model from ultralytics framework (PyTorch)** :\n",
        "\n",
        "Load the pre-trained YOLO model from the ultralytics library. This library provides development tools for the YOLO (You Only Look Once) series of computer vision models. You are allowed to select the currently validated Pose model in `model_name`."
      ],
      "metadata": {
        "id": "LAnu0v5W2IWP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n0wrZuWEgAdI",
        "outputId": "2f309a03-54cf-4005-b8c9-31b441074031"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ultralytics\n",
            "  Downloading ultralytics-8.3.32-py3-none-any.whl.metadata (35 kB)\n",
            "Requirement already satisfied: numpy>=1.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.26.4)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (3.8.0)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.10.0.84)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (11.0.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.32.3)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.13.1)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.20.1+cu121)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.66.6)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.0.0)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.2.2)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.13.2)\n",
            "Collecting ultralytics-thop>=2.0.0 (from ultralytics)\n",
            "  Downloading ultralytics_thop-2.0.11-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2024.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2024.8.30)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.8.0->ultralytics) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\n",
            "Downloading ultralytics-8.3.32-py3-none-any.whl (887 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m887.0/887.0 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ultralytics_thop-2.0.11-py3-none-any.whl (26 kB)\n",
            "Installing collected packages: ultralytics-thop, ultralytics\n",
            "Successfully installed ultralytics-8.3.32 ultralytics-thop-2.0.11\n",
            "Creating new Ultralytics Settings v0.0.6 file ‚úÖ \n",
            "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
            "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n",
            "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8n-pose.pt to 'yolov8n-pose.pt'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6.52M/6.52M [00:00<00:00, 122MB/s]\n"
          ]
        }
      ],
      "source": [
        "!pip install ultralytics\n",
        "from ultralytics import YOLO\n",
        "\n",
        "model_name = \"yolov8n-pose.pt\"   # @param [\"yolov8n-pose.pt\", \"yolov8s-pose.pt\", \"yolov8m-pose.pt\", \"yolov8l-pose.pt\", \"yolov8x-pose.pt\", \"yolov8x-pose-p6.pt\"]\n",
        "model = YOLO(model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Delegation"
      ],
      "metadata": {
        "id": "kO4IxdRniSMC"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E154FO6wOn1C"
      },
      "source": [
        "## HailoRT"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step1.** Export to ONNX format with opset 11."
      ],
      "metadata": {
        "id": "p64T8ejZlc57"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output_path = model.export(format='onnx', imgsz=640, opset=11)\n",
        "output_path"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 894
        },
        "id": "JeDpCEfliPZh",
        "outputId": "69c3cf0d-1042-494f-beb0-de03f4b2f21e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ultralytics 8.3.32 üöÄ Python-3.10.12 torch-2.5.1+cu121 CPU (Intel Xeon 2.20GHz)\n",
            "YOLOv8n-pose summary (fused): 187 layers, 3,289,964 parameters, 0 gradients, 9.2 GFLOPs\n",
            "\n",
            "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'yolov8n-pose.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 56, 8400) (6.5 MB)\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirements ['onnx>=1.12.0', 'onnxslim', 'onnxruntime'] not found, attempting AutoUpdate...\n",
            "Collecting onnx>=1.12.0\n",
            "  Downloading onnx-1.17.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
            "Collecting onnxslim\n",
            "  Downloading onnxslim-0.1.39-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting onnxruntime\n",
            "  Downloading onnxruntime-1.20.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from onnx>=1.12.0) (1.26.4)\n",
            "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from onnx>=1.12.0) (4.25.5)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxslim) (1.13.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from onnxslim) (24.2)\n",
            "Collecting coloredlogs (from onnxruntime)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (24.3.25)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxslim) (1.3.0)\n",
            "Downloading onnx-1.17.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.0 MB)\n",
            "   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 16.0/16.0 MB 101.4 MB/s eta 0:00:00\n",
            "Downloading onnxslim-0.1.39-py3-none-any.whl (141 kB)\n",
            "   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 141.8/141.8 kB 176.9 MB/s eta 0:00:00\n",
            "Downloading onnxruntime-1.20.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (13.3 MB)\n",
            "   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 13.3/13.3 MB 112.6 MB/s eta 0:00:00\n",
            "Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 46.0/46.0 kB 142.5 MB/s eta 0:00:00\n",
            "Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 86.8/86.8 kB 176.4 MB/s eta 0:00:00\n",
            "Installing collected packages: onnx, humanfriendly, onnxslim, coloredlogs, onnxruntime\n",
            "Successfully installed coloredlogs-15.0.1 humanfriendly-10.0 onnx-1.17.0 onnxruntime-1.20.0 onnxslim-0.1.39\n",
            "\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m AutoUpdate success ‚úÖ 13.0s, installed 3 packages: ['onnx>=1.12.0', 'onnxslim', 'onnxruntime']\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m ‚ö†Ô∏è \u001b[1mRestart runtime or rerun command for updates to take effect\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.17.0 opset 11...\n",
            "\u001b[34m\u001b[1mONNX:\u001b[0m slimming with onnxslim 0.1.39...\n",
            "\u001b[34m\u001b[1mONNX:\u001b[0m export success ‚úÖ 15.3s, saved as 'yolov8n-pose.onnx' (12.8 MB)\n",
            "\n",
            "Export complete (16.6s)\n",
            "Results saved to \u001b[1m/content\u001b[0m\n",
            "Predict:         yolo predict task=pose model=yolov8n-pose.onnx imgsz=640  \n",
            "Validate:        yolo val task=pose model=yolov8n-pose.onnx imgsz=640 data=/usr/src/app/ultralytics/datasets/coco-pose.yaml  \n",
            "Visualize:       https://netron.app\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'yolov8n-pose.onnx'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step2.** Validate the accuracy"
      ],
      "metadata": {
        "id": "oy7-mmwalRUu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "delegated_model = YOLO(output_path)\n",
        "metrics = delegated_model.val(data=\"coco8-pose.yaml\")\n",
        "\n",
        "print(metrics.box.map)  # mAP50-95\n",
        "print(metrics.box.map50)\n",
        "print(metrics.pose.map)  # mAP50-95\n",
        "print(metrics.pose.map50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ux_Q3MtjLMC",
        "outputId": "8bdbb431-f35e-457e-9517-d856c1c39b1a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ultralytics 8.3.32 üöÄ Python-3.10.12 torch-2.5.1+cu121 CPU (Intel Xeon 2.20GHz)\n",
            "Loading yolov8n-pose.onnx for ONNX Runtime inference...\n",
            "Preferring ONNX Runtime AzureExecutionProvider\n",
            "Setting batch=1 input of shape (1, 3, 640, 640)\n",
            "\n",
            "Dataset 'coco8-pose.yaml' images not found ‚ö†Ô∏è, missing path '/content/datasets/coco8-pose/images/val'\n",
            "Downloading https://ultralytics.com/assets/coco8-pose.zip to '/content/datasets/coco8-pose.zip'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 334k/334k [00:00<00:00, 37.3MB/s]\n",
            "Unzipping /content/datasets/coco8-pose.zip to /content/datasets/coco8-pose...: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 27/27 [00:00<00:00, 3510.86file/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset download success ‚úÖ (0.2s), saved to \u001b[1m/content/datasets\u001b[0m\n",
            "\n",
            "Downloading https://ultralytics.com/assets/Arial.ttf to '/root/.config/Ultralytics/Arial.ttf'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 755k/755k [00:00<00:00, 106MB/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/datasets/coco8-pose/labels/val... 4 images, 0 backgrounds, 0 corrupt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:00<00:00, 106.12it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/datasets/coco8-pose/labels/val.cache\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:01<00:00,  2.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          4         14      0.966      0.929      0.929      0.751      0.895      0.714      0.714      0.393\n",
            "Speed: 8.4ms preprocess, 345.8ms inference, 0.0ms loss, 7.6ms postprocess per image\n",
            "Results saved to \u001b[1mruns/pose/val\u001b[0m\n",
            "0.7508437068003383\n",
            "0.9293362831858406\n",
            "0.39309694566278164\n",
            "0.7143203636363634\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cYYI4xlYOq6e"
      },
      "source": [
        "**Step3.** Reproduce the output process manually."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "bxoukUHjcaSq"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class YOLOv8Pose():\n",
        "    def __init__(self, model_path, nc = 1, num_of_keypoints = 17):\n",
        "        self.session = ort.InferenceSession(model_path)\n",
        "        self.input_details  = [i for i in self.session.get_inputs()]\n",
        "        self.output_details = [i.name for i in self.session.get_outputs()]\n",
        "        self.X_axis = [0, 2] + [5 + i * 3 for i in range(num_of_keypoints)]\n",
        "        self.y_axis = [1, 3] + [6 + i * 3 for i in range(num_of_keypoints)]\n",
        "        self.nc = nc\n",
        "\n",
        "    def predict(self, frames, conf=0.25, iou=0.7, agnostic=False, max_det=300):\n",
        "        im = self.preprocess(frames)\n",
        "        preds = self.inference(im)\n",
        "        results = self.postprocess(preds, conf_thres=conf, iou_thres=iou, agnostic=agnostic, max_det=max_det)\n",
        "        return results\n",
        "\n",
        "    def postprocess(self, preds, conf_thres=0.25, iou_thres=0.45, classes=None, agnostic=False, multi_label=False, labels=(), max_det=300, nc=0, max_time_img=0.05, max_nms=30000, max_wh=7680, in_place=True, rotated=False):\n",
        "        xc = np.max(preds[:, 4: self.nc + 4], axis = 1) > conf_thres\n",
        "        preds = np.transpose(preds, (0, 2, 1))\n",
        "        preds[..., :4] = xywh2xyxy(preds[..., :4])\n",
        "        x = preds[0][xc[0]]\n",
        "\n",
        "        if not x.shape[0]:\n",
        "          return None\n",
        "        box, cls, keypoints = x[:, :4], x[:, 4:5], x[:, 5:]\n",
        "        j = np.argmax(cls, axis=1)\n",
        "        conf = cls[[i for i in range(len(j))], j]\n",
        "        concatenated = np.concatenate((box, conf.reshape(-1, 1), j.reshape(-1, 1).astype(float), keypoints), axis=1)\n",
        "        x = concatenated[conf.flatten() > conf_thres]\n",
        "\n",
        "        if x.shape[0] > max_nms:  # excess boxes\n",
        "            x = x[x[:, 4].argsort(descending=True)[:max_nms]]\n",
        "        cls = x[:, 5:6] * (0 if agnostic else max_wh)\n",
        "        scores, boxes = x[:, 4], x[:, :4] + cls\n",
        "\n",
        "        i = non_max_suppression(boxes, scores, iou_thres)\n",
        "        return [x[i[:max_det]]]\n",
        "\n",
        "    def inference(self, im):\n",
        "        inputs = {key.name: value for key, value in zip(self.input_details, [im])}\n",
        "        preds = self.session.run(self.output_details, inputs)[0]\n",
        "        return preds\n",
        "\n",
        "    def preprocess(self, im):\n",
        "        im = np.stack(self.pre_transform(im))\n",
        "        im = im[..., ::-1]\n",
        "        im = np.ascontiguousarray(im).astype(np.float32)\n",
        "        im /= 255.0\n",
        "        im = np.transpose(im, (0, 3, 1, 2))\n",
        "        return im\n",
        "\n",
        "    def pre_transform(self, im):\n",
        "        imgsz = self.input_details[0].shape[2:4]\n",
        "        return [cv2.resize(im[0], imgsz, interpolation=cv2.INTER_LINEAR) for x in im]\n",
        "\n",
        "class LetterBox:\n",
        "    def __init__(self, new_shape=(640, 640), auto=False, scaleFill=False, scaleup=True, center=True, stride=32):\n",
        "        self.new_shape = new_shape\n",
        "        self.auto = auto\n",
        "        self.scaleFill = scaleFill\n",
        "        self.scaleup = scaleup\n",
        "        self.stride = stride\n",
        "        self.center = center\n",
        "\n",
        "    def __call__(self, labels=None, image=None):\n",
        "        if labels is None:\n",
        "            labels = {}\n",
        "        img = labels.get(\"img\") if image is None else image\n",
        "        shape = img.shape[:2]\n",
        "        new_shape = labels.pop(\"rect_shape\", self.new_shape)\n",
        "        if isinstance(new_shape, int):\n",
        "            new_shape = (new_shape, new_shape)\n",
        "\n",
        "        r = min(new_shape[0] / shape[0], new_shape[1] / shape[1])\n",
        "        if not self.scaleup:\n",
        "            r = min(r, 1.0)\n",
        "\n",
        "        ratio = r, r\n",
        "        new_unpad = int(round(shape[1] * r)), int(round(shape[0] * r))\n",
        "        dw, dh = new_shape[1] - new_unpad[0], new_shape[0] - new_unpad[1]  # wh padding\n",
        "        if self.auto:\n",
        "            dw, dh = np.mod(dw, self.stride), np.mod(dh, self.stride)  # wh padding\n",
        "        elif self.scaleFill:\n",
        "            dw, dh = 0.0, 0.0\n",
        "            new_unpad = (new_shape[1], new_shape[0])\n",
        "            ratio = new_shape[1] / shape[1], new_shape[0] / shape[0]  # width, height ratios\n",
        "\n",
        "        if self.center:\n",
        "            dw /= 2\n",
        "            dh /= 2\n",
        "\n",
        "        if shape[::-1] != new_unpad:\n",
        "            img = cv2.resize(img, new_unpad, interpolation=cv2.INTER_LINEAR)\n",
        "        top, bottom = int(round(dh - 0.1)) if self.center else 0, int(round(dh + 0.1))\n",
        "        left, right = int(round(dw - 0.1)) if self.center else 0, int(round(dw + 0.1))\n",
        "        img = cv2.copyMakeBorder(img, top, bottom, left, right, cv2.BORDER_CONSTANT, value=(114, 114, 114))\n",
        "        if labels.get(\"ratio_pad\"):\n",
        "            labels[\"ratio_pad\"] = (labels[\"ratio_pad\"], (left, top))  # for evaluation\n",
        "\n",
        "        if len(labels):\n",
        "            labels = self._update_labels(labels, ratio, dw, dh)\n",
        "            labels[\"img\"] = img\n",
        "            labels[\"resized_shape\"] = new_shape\n",
        "            return labels\n",
        "        else:\n",
        "            return img\n",
        "\n",
        "    def _update_labels(self, labels, ratio, padw, padh):\n",
        "        labels[\"instances\"].convert_bbox(format=\"xyxy\")\n",
        "        labels[\"instances\"].denormalize(*labels[\"img\"].shape[:2][::-1])\n",
        "        labels[\"instances\"].scale(*ratio)\n",
        "        labels[\"instances\"].add_padding(padw, padh)\n",
        "        return labels\n",
        "\n",
        "def xywh2xyxy(x):\n",
        "    assert x.shape[-1] == 4, f\"input shape last dimension expected 4 but input shape is {x.shape}\"\n",
        "    y = np.empty_like(x)\n",
        "    xy = x[..., :2]\n",
        "    wh = x[..., 2:] / 2\n",
        "    y[..., :2] = xy - wh\n",
        "    y[..., 2:] = xy + wh\n",
        "    return y\n",
        "\n",
        "def non_max_suppression(boxes, scores, iou_threshold):\n",
        "    x1 = boxes[:, 0]\n",
        "    y1 = boxes[:, 1]\n",
        "    x2 = boxes[:, 2]\n",
        "    y2 = boxes[:, 3]\n",
        "\n",
        "    areas = (x2 - x1) * (y2 - y1)\n",
        "    order = scores.argsort()[::-1]\n",
        "\n",
        "    keep = []\n",
        "    while order.size > 0:\n",
        "        i = order[0]\n",
        "        keep.append(i)\n",
        "        xx1 = np.maximum(x1[i], x1[order[1:]])\n",
        "        yy1 = np.maximum(y1[i], y1[order[1:]])\n",
        "        xx2 = np.minimum(x2[i], x2[order[1:]])\n",
        "        yy2 = np.minimum(y2[i], y2[order[1:]])\n",
        "        w = np.maximum(0.0, xx2 - xx1)\n",
        "        h = np.maximum(0.0, yy2 - yy1)\n",
        "        inter = w * h\n",
        "        iou = inter / (areas[i] + areas[order[1:]] - inter)\n",
        "        inds = np.where(iou <= iou_threshold)[0]\n",
        "        order = order[inds + 1]\n",
        "\n",
        "    return np.array(keep)\n",
        "\n",
        "def plot(image, results, connections, visible = 0.1):\n",
        "    for bboxes in results:\n",
        "      x1, y1, x2, y2 = int(bboxes[0] * image.shape[1] / 640), int(bboxes[1] * image.shape[0] / 640), int(bboxes[2] * image.shape[1] / 640), int(bboxes[3] * image.shape[0] / 640)\n",
        "      conf, cls = bboxes[4] , bboxes[5]\n",
        "      cv2.rectangle(image, (x1, y1), (x2, y2), color=(0, 255, 0), thickness=3)\n",
        "      cv2.putText(image, f'instance {conf:.2f}', (x1, y1 - 2), 0, 1, [0, 255, 0], thickness=2, lineType=cv2.LINE_AA)\n",
        "\n",
        "      keypoints = bboxes[6:].reshape(-1, 3)\n",
        "      keypoints[:, 0] *=  image.shape[1] / 640\n",
        "      keypoints[:, 1] *=  image.shape[0] / 640\n",
        "      for i in range(len(keypoints)):\n",
        "        if keypoints[i][2] > visible:\n",
        "          cv2.circle(image, (int(keypoints[i][0]), int(keypoints[i][1])), 5, (0, 255, 0), -1)\n",
        "\n",
        "      for connection in connections:\n",
        "        start_idx, end_idx = connection\n",
        "        if start_idx < len(keypoints) and end_idx < len(keypoints):\n",
        "          if keypoints[start_idx][2] > visible and keypoints[end_idx][2] > visible:\n",
        "            cv2.line(image, (int(keypoints[start_idx][0]), int(keypoints[start_idx][1])), (int(keypoints[end_idx][0]), int(keypoints[end_idx][1])), (0, 255, 0), 2)\n",
        "    return image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s99rScYrAfwR"
      },
      "outputs": [],
      "source": [
        "!wget https://ultralytics.com/images/bus.jpg -O bus.jpg\n",
        "import onnxruntime as ort\n",
        "\n",
        "connections = [(4, 2), (2, 0), (3, 1), (1, 0), (0, 6), (6, 8), (8, 10),\n",
        "         (0, 5), (5, 7), (7, 9), (0, 12), (12, 14), (14, 16), (0, 11), (11, 13), (13, 15)]\n",
        "\n",
        "img = cv2.imread('./bus.jpg')\n",
        "onnx_model = YOLOv8Pose(model_path='./yolov8n-pose.onnx', num_of_keypoints = 17)\n",
        "results = onnx_model.predict([img])\n",
        "plt.imshow(plot(img.copy(), results[0].copy(), connections)[:, :, ::-1])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Export to Others"
      ],
      "metadata": {
        "id": "1GgvgeoSlG49"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AzfsnTq9P4jc"
      },
      "source": [
        "**Step1.** Export to TFLite format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YwymNPxWP4qO"
      },
      "outputs": [],
      "source": [
        "output_path = model.export(format='tflite', imgsz=640)\n",
        "output_path"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step2.** Validate the accuracy"
      ],
      "metadata": {
        "id": "gkYOJWSjIkDR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "delegated_model = YOLO('./yolov8n-pose_saved_model/yolov8n-pose_float32.tflite')\n",
        "metrics = delegated_model.val(data=\"coco8-pose.yaml\")\n",
        "\n",
        "print(metrics.box.map)  # mAP50-95\n",
        "print(metrics.box.map50)\n",
        "print(metrics.pose.map)  # mAP50-95\n",
        "print(metrics.pose.map50)"
      ],
      "metadata": {
        "id": "PEDPrc6al_bH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "delegated_model = YOLO('./yolov8n-pose_saved_model/yolov8n-pose_float16.tflite')\n",
        "metrics = delegated_model.val(data=\"coco8-pose.yaml\")\n",
        "\n",
        "print(metrics.box.map)  # mAP50-95\n",
        "print(metrics.box.map50)\n",
        "print(metrics.pose.map)  # mAP50-95\n",
        "print(metrics.pose.map50)"
      ],
      "metadata": {
        "id": "wNsYptt_mAAu"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}