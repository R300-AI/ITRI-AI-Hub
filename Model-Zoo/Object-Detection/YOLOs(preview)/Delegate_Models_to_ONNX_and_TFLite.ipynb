{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZTwwSjCE8ys"
      },
      "source": [
        "# Delegate YOLOs to ONNX and TFLite Format\n",
        "\n",
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a href=\"https://colab.research.google.com/drive/1Rw8vP0_bh8A2r0cssGAkuC7UyFk8MJac?usp=sharing\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> Run in Colab\n",
        "    </a>\n",
        "  </td>\n",
        "</table>\n",
        "<br>\n",
        "</table>\n",
        "<br>\n",
        "\n",
        "**Import model from ultralytics framework (PyTorch)** :\n",
        "\n",
        "Load the pre-trained YOLO model from the ultralytics library. This library provides development tools for the YOLO (You Only Look Once) series of computer vision models. You are allowed to select the currently validated Object Detection model in `model_name`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OMjHRw8CVQYu",
        "outputId": "510b630e-cf25-4de6-ff94-50872c5e2b31"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ultralytics in /usr/local/lib/python3.10/dist-packages (8.3.40)\n",
            "Requirement already satisfied: numpy>=1.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.26.4)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (3.8.0)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.10.0.84)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (11.0.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.32.3)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.13.1)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.20.1+cu121)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.66.6)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.0.0)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.2.2)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.13.2)\n",
            "Requirement already satisfied: ultralytics-thop>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.0.12)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.55.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2024.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2024.8.30)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.8.0->ultralytics) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\n",
            "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11x.pt to 'yolo11x.pt'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 109M/109M [00:00<00:00, 212MB/s]\n"
          ]
        }
      ],
      "source": [
        "!pip install ultralytics\n",
        "from ultralytics import YOLO\n",
        "\n",
        "model_name = \"yolo11x.pt\"   # @param [\"yolov5nu.pt\", \"yolov5su.pt\", \"yolov5mu.pt\", \"yolov5lu.pt\", \"yolov5xu.pt\", \"yolov8n.pt\", \"yolov8s.pt\", \"yolov8m.pt\", \"yolov8l.pt\", \"yolov8x.pt\", \"yolo11n.pt\", \"yolo11s.pt\", \"yolo11m.pt\", \"yolo11l.pt\", \"yolo11x.pt\"]\n",
        "model = YOLO(model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PWc4DxZ7G1Jz"
      },
      "source": [
        "# Delegation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ozr-DhWiNCQi"
      },
      "source": [
        "## Export to ONNX\n",
        "Usage: [[KleidiAI]](https://github.com/R300-AI/ITRI-AI-Hub/tree/main/Model-Zoo/Object-Detection/YOLOs/KleidiAI)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4VsOGn3vMFFO"
      },
      "source": [
        "**Step1.** Export the model to ONNX format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "hmmT3fcLJ9ke",
        "outputId": "4f4e82f8-920c-43f1-9382-426b578cd338"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ultralytics 8.3.40 ðŸš€ Python-3.10.12 torch-2.5.1+cu121 CPU (Intel Xeon 2.30GHz)\n",
            "YOLO11x summary (fused): 464 layers, 56,919,424 parameters, 0 gradients, 194.9 GFLOPs\n",
            "\n",
            "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'yolo11x.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 84, 8400) (109.3 MB)\n",
            "\n",
            "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.17.0 opset 19...\n",
            "\u001b[34m\u001b[1mONNX:\u001b[0m slimming with onnxslim 0.1.42...\n",
            "\u001b[34m\u001b[1mONNX:\u001b[0m export success âœ… 10.7s, saved as 'yolo11x.onnx' (217.5 MB)\n",
            "\n",
            "Export complete (28.2s)\n",
            "Results saved to \u001b[1m/content\u001b[0m\n",
            "Predict:         yolo predict task=detect model=yolo11x.onnx imgsz=640  \n",
            "Validate:        yolo val task=detect model=yolo11x.onnx imgsz=640 data=/ultralytics/ultralytics/cfg/datasets/coco.yaml  \n",
            "Visualize:       https://netron.app\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'yolo11x.onnx'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "source": [
        "output_path = model.export(format='onnx', imgsz=640)  # delegating to hailo requred opset=11\n",
        "output_path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "edXkGMKmMF1x"
      },
      "source": [
        "**Step2.** Validate the accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mR-D0jEOMKY_",
        "outputId": "8016a449-a69c-4782-d730-ba5eb258a160"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING âš ï¸ Unable to automatically guess model task, assuming 'task=detect'. Explicitly define task for your model, i.e. 'task=detect', 'segment', 'classify','pose' or 'obb'.\n",
            "Ultralytics 8.3.39 ðŸš€ Python-3.10.12 torch-2.5.1+cu121 CPU (Intel Xeon 2.20GHz)\n",
            "Loading yolov8n_float32.onnx for ONNX Runtime inference...\n",
            "Preferring ONNX Runtime AzureExecutionProvider\n",
            "Setting batch=1 input of shape (1, 3, 640, 640)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/datasets/coco8/labels/val.cache... 4 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<?, ?it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00,  5.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          4         17      0.708      0.833      0.809      0.609\n",
            "                person          3         10      0.781        0.5      0.541      0.278\n",
            "                   dog          1          1      0.451          1      0.497      0.348\n",
            "                 horse          1          2      0.817          1      0.995      0.747\n",
            "              elephant          1          2      0.896        0.5      0.828      0.392\n",
            "              umbrella          1          1      0.635          1      0.995      0.995\n",
            "          potted plant          1          1      0.667          1      0.995      0.895\n",
            "Speed: 7.0ms preprocess, 160.9ms inference, 0.0ms loss, 2.0ms postprocess per image\n",
            "Results saved to \u001b[1mruns/detect/val2\u001b[0m\n",
            "0.6092337187926128\n",
            "0.808564761582811\n"
          ]
        }
      ],
      "source": [
        "delegated_model = YOLO(output_path.replace('.onnx', '_float32.onnx'))\n",
        "metrics = delegated_model.val(data=\"coco8.yaml\")\n",
        "\n",
        "print(metrics.box.map)  # mAP50-95\n",
        "print(metrics.box.map50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9NXyPT3rMNP4"
      },
      "source": [
        "**Step3.** Reproduce the output process manually."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tYQcXZuUFqzq"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class YOLOs():\n",
        "    def __init__(self, model_path, nc = 1, num_of_keypoints = 17):\n",
        "        self.session = ort.InferenceSession(model_path)\n",
        "        self.input_type = self.session.get_inputs()[0].type\n",
        "        self.input_details  = [i for i in self.session.get_inputs()]\n",
        "        self.output_details = [i.name for i in self.session.get_outputs()]\n",
        "        self.io = self.session.io_binding()\n",
        "        self.io.bind_output(self.output_details[0])\n",
        "\n",
        "        self.X_axis = [0, 2] + [5 + i * 3 for i in range(num_of_keypoints)]\n",
        "        self.y_axis = [1, 3] + [6 + i * 3 for i in range(num_of_keypoints)]\n",
        "        self.nc = nc\n",
        "\n",
        "    def predict(self, frames, conf=0.25, iou=0.7, agnostic=False, max_det=300):\n",
        "        im = self.preprocess(frames)\n",
        "        preds = self.inference(im.astype(np.float16)) if self.input_type == 'tensor(float16)' else self.inference(im)\n",
        "        results = self.postprocess(preds, conf_thres=conf, iou_thres=iou, agnostic=agnostic, max_det=max_det)\n",
        "        return results\n",
        "\n",
        "    def postprocess(self, preds, conf_thres=0.25, iou_thres=0.45, classes=None, agnostic=False, multi_label=False, labels=(), max_det=300, nc=0, max_time_img=0.05, max_nms=30000, max_wh=7680, in_place=True, rotated=False):\n",
        "        xc = np.max(preds[:, 4: self.nc + 4], axis = 1) > conf_thres\n",
        "        preds = np.transpose(preds, (0, 2, 1))\n",
        "        preds[..., :4] = xywh2xyxy(preds[..., :4])\n",
        "        x = preds[0][xc[0]]\n",
        "\n",
        "        if not x.shape[0]:\n",
        "          return None\n",
        "        box, cls, keypoints = x[:, :4], x[:, 4:5], x[:, 5:]\n",
        "        j = np.argmax(cls, axis=1)\n",
        "        conf = cls[[i for i in range(len(j))], j]\n",
        "        concatenated = np.concatenate((box, conf.reshape(-1, 1), j.reshape(-1, 1).astype(float), keypoints), axis=1)\n",
        "        x = concatenated[conf.flatten() > conf_thres]\n",
        "\n",
        "        if x.shape[0] > max_nms:  # excess boxes\n",
        "            x = x[x[:, 4].argsort(descending=True)[:max_nms]]\n",
        "        cls = x[:, 5:6] * (0 if agnostic else max_wh)\n",
        "        scores, boxes = x[:, 4], x[:, :4] + cls\n",
        "\n",
        "        i = non_max_suppression(boxes, scores, iou_thres)\n",
        "        return [x[i[:max_det]]]\n",
        "\n",
        "    def inference(self, im):\n",
        "        self.io.bind_cpu_input(self.input_details[0].name, im)\n",
        "        self.session.run_with_iobinding(self.io)\n",
        "        return self.io.copy_outputs_to_cpu()[0]\n",
        "\n",
        "    def preprocess(self, im):\n",
        "        im = np.stack(self.pre_transform(im))\n",
        "        im = im[..., ::-1]\n",
        "        im = np.ascontiguousarray(im).astype(np.float32)\n",
        "        im /= 255.0\n",
        "        im = np.transpose(im, (0, 3, 1, 2))\n",
        "        return im\n",
        "\n",
        "    def pre_transform(self, im):\n",
        "        imgsz = self.input_details[0].shape[2:]\n",
        "        return [cv2.resize(im[0], imgsz, interpolation=cv2.INTER_LINEAR) for x in im]\n",
        "\n",
        "class LetterBox:\n",
        "    def __init__(self, new_shape=(640, 640), auto=False, scaleFill=False, scaleup=True, center=True, stride=32):\n",
        "        self.new_shape = new_shape\n",
        "        self.auto = auto\n",
        "        self.scaleFill = scaleFill\n",
        "        self.scaleup = scaleup\n",
        "        self.stride = stride\n",
        "        self.center = center\n",
        "\n",
        "    def __call__(self, labels=None, image=None):\n",
        "        if labels is None:\n",
        "            labels = {}\n",
        "        img = labels.get(\"img\") if image is None else image\n",
        "        shape = img.shape[:2]\n",
        "        new_shape = labels.pop(\"rect_shape\", self.new_shape)\n",
        "        if isinstance(new_shape, int):\n",
        "            new_shape = (new_shape, new_shape)\n",
        "\n",
        "        r = min(new_shape[0] / shape[0], new_shape[1] / shape[1])\n",
        "        if not self.scaleup:\n",
        "            r = min(r, 1.0)\n",
        "\n",
        "        ratio = r, r\n",
        "        new_unpad = int(round(shape[1] * r)), int(round(shape[0] * r))\n",
        "        dw, dh = new_shape[1] - new_unpad[0], new_shape[0] - new_unpad[1]  # wh padding\n",
        "        if self.auto:\n",
        "            dw, dh = np.mod(dw, self.stride), np.mod(dh, self.stride)  # wh padding\n",
        "        elif self.scaleFill:\n",
        "            dw, dh = 0.0, 0.0\n",
        "            new_unpad = (new_shape[1], new_shape[0])\n",
        "            ratio = new_shape[1] / shape[1], new_shape[0] / shape[0]  # width, height ratios\n",
        "\n",
        "        if self.center:\n",
        "            dw /= 2\n",
        "            dh /= 2\n",
        "\n",
        "        if shape[::-1] != new_unpad:\n",
        "            img = cv2.resize(img, new_unpad, interpolation=cv2.INTER_LINEAR)\n",
        "        top, bottom = int(round(dh - 0.1)) if self.center else 0, int(round(dh + 0.1))\n",
        "        left, right = int(round(dw - 0.1)) if self.center else 0, int(round(dw + 0.1))\n",
        "        img = cv2.copyMakeBorder(img, top, bottom, left, right, cv2.BORDER_CONSTANT, value=(114, 114, 114))\n",
        "        if labels.get(\"ratio_pad\"):\n",
        "            labels[\"ratio_pad\"] = (labels[\"ratio_pad\"], (left, top))  # for evaluation\n",
        "\n",
        "        if len(labels):\n",
        "            labels = self._update_labels(labels, ratio, dw, dh)\n",
        "            labels[\"img\"] = img\n",
        "            labels[\"resized_shape\"] = new_shape\n",
        "            return labels\n",
        "        else:\n",
        "            return img\n",
        "\n",
        "    def _update_labels(self, labels, ratio, padw, padh):\n",
        "        labels[\"instances\"].convert_bbox(format=\"xyxy\")\n",
        "        labels[\"instances\"].denormalize(*labels[\"img\"].shape[:2][::-1])\n",
        "        labels[\"instances\"].scale(*ratio)\n",
        "        labels[\"instances\"].add_padding(padw, padh)\n",
        "        return labels\n",
        "\n",
        "def plot(image, results, labels):\n",
        "    for bboxes in results:\n",
        "      x1, y1, x2, y2 = int(bboxes[0] * image.shape[1] / 640), int(bboxes[1] * image.shape[0] / 640), int(bboxes[2] * image.shape[1] / 640), int(bboxes[3] * image.shape[0] / 640)\n",
        "      conf, cls = bboxes[4] , bboxes[5]\n",
        "      cv2.rectangle(image, (x1, y1), (x2, y2), color=(0, 255, 0), thickness=3)\n",
        "      cv2.putText(image, f'{labels[int(cls)]} {conf:.2f}', (x1, y1 - 2), 0, 1, [0, 255, 0], thickness=2, lineType=cv2.LINE_AA)\n",
        "    return image\n",
        "\n",
        "def xywh2xyxy(x):\n",
        "    assert x.shape[-1] == 4, f\"input shape last dimension expected 4 but input shape is {x.shape}\"\n",
        "    y = np.empty_like(x)\n",
        "    xy = x[..., :2]\n",
        "    wh = x[..., 2:] / 2\n",
        "    y[..., :2] = xy - wh\n",
        "    y[..., 2:] = xy + wh\n",
        "    return y\n",
        "\n",
        "def non_max_suppression(boxes, scores, iou_threshold):\n",
        "    x1 = boxes[:, 0]\n",
        "    y1 = boxes[:, 1]\n",
        "    x2 = boxes[:, 2]\n",
        "    y2 = boxes[:, 3]\n",
        "\n",
        "    areas = (x2 - x1) * (y2 - y1)\n",
        "    order = scores.argsort()[::-1]\n",
        "\n",
        "    keep = []\n",
        "    while order.size > 0:\n",
        "        i = order[0]\n",
        "        keep.append(i)\n",
        "        xx1 = np.maximum(x1[i], x1[order[1:]])\n",
        "        yy1 = np.maximum(y1[i], y1[order[1:]])\n",
        "        xx2 = np.minimum(x2[i], x2[order[1:]])\n",
        "        yy2 = np.minimum(y2[i], y2[order[1:]])\n",
        "        w = np.maximum(0.0, xx2 - xx1)\n",
        "        h = np.maximum(0.0, yy2 - yy1)\n",
        "        inter = w * h"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EEoVNbrVJvr2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 827
        },
        "outputId": "a8f2592b-f6ac-4d31-c060-4e602e1e908b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-12-02 07:26:59--  https://ultralytics.com/images/bus.jpg\n",
            "Resolving ultralytics.com (ultralytics.com)... 99.83.190.102, 75.2.70.75\n",
            "Connecting to ultralytics.com (ultralytics.com)|99.83.190.102|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://www.ultralytics.com/images/bus.jpg [following]\n",
            "--2024-12-02 07:26:59--  https://www.ultralytics.com/images/bus.jpg\n",
            "Resolving www.ultralytics.com (www.ultralytics.com)... 104.18.40.102, 172.64.147.154, 2606:4700:4400::ac40:939a, ...\n",
            "Connecting to www.ultralytics.com (www.ultralytics.com)|104.18.40.102|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://github.com/ultralytics/assets/releases/download/v0.0.0/bus.jpg [following]\n",
            "--2024-12-02 07:26:59--  https://github.com/ultralytics/assets/releases/download/v0.0.0/bus.jpg\n",
            "Resolving github.com (github.com)... 140.82.116.4\n",
            "Connecting to github.com (github.com)|140.82.116.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/521807533/8d96d999-9a94-4cfc-ba1d-7c35cc3e45cc?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20241202%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20241202T072700Z&X-Amz-Expires=300&X-Amz-Signature=93ded6036dcd6db340927256262d2502141ce333a09df7326b8dc496692cfe39&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dbus.jpg&response-content-type=application%2Foctet-stream [following]\n",
            "--2024-12-02 07:27:00--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/521807533/8d96d999-9a94-4cfc-ba1d-7c35cc3e45cc?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20241202%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20241202T072700Z&X-Amz-Expires=300&X-Amz-Signature=93ded6036dcd6db340927256262d2502141ce333a09df7326b8dc496692cfe39&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dbus.jpg&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 137419 (134K) [application/octet-stream]\n",
            "Saving to: â€˜bus.jpgâ€™\n",
            "\n",
            "bus.jpg             100%[===================>] 134.20K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2024-12-02 07:27:00 (6.92 MB/s) - â€˜bus.jpgâ€™ saved [137419/137419]\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-68f1d76353e3>\u001b[0m in \u001b[0;36m<cell line: 17>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./bus.jpg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0monnx_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mYOLOs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_path\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.onnx'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_float16.onnx'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0monnx_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.25\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miou\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magnostic\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_det\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-af2a693731f5>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, frames, conf, iou, agnostic, max_det)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'tensor(float16)'\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpostprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf_thres\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miou_thres\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0miou\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magnostic\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0magnostic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_det\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_det\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-af2a693731f5>\u001b[0m in \u001b[0;36mpostprocess\u001b[0;34m(self, preds, conf_thres, iou_thres, classes, agnostic, multi_label, labels, max_det, nc, max_time_img, max_nms, max_wh, in_place, rotated)\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mboxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnon_max_suppression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mboxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miou_thres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mmax_det\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-af2a693731f5>\u001b[0m in \u001b[0;36mnon_max_suppression\u001b[0;34m(boxes, scores, iou_threshold)\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0mkeep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0mxx1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaximum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m         \u001b[0myy1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaximum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0mxx2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "!wget https://ultralytics.com/images/bus.jpg -O bus.jpg\n",
        "import onnxruntime as ort\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "\n",
        "labels = {0: 'person', 1: 'bicycle', 2: 'car', 3: 'motorcycle', 4: 'airplane', 5: 'bus', 6: 'train', 7: 'truck', 8: 'boat', 9: 'traffic light', 10: 'fire hydrant',\n",
        "      11: 'stop sign', 12: 'parking meter', 13: 'bench', 14: 'bird', 15: 'cat', 16: 'dog', 17: 'horse', 18: 'sheep', 19: 'cow', 20: 'elephant', 21: 'bear',\n",
        "      22: 'zebra', 23: 'giraffe', 24: 'backpack', 25: 'umbrella', 26: 'handbag', 27: 'tie', 28: 'suitcase', 29: 'frisbee', 30: 'skis', 31: 'snowboard', 32: 'sports ball',\n",
        "      33: 'kite', 34: 'baseball bat', 35: 'baseball glove', 36: 'skateboard', 37: 'surfboard', 38: 'tennis racket', 39: 'bottle', 40: 'wine glass', 41: 'cup', 42: 'fork',\n",
        "      43: 'knife', 44: 'spoon', 45: 'bowl', 46: 'banana', 47: 'apple', 48: 'sandwich', 49: 'orange', 50: 'broccoli', 51: 'carrot', 52: 'hot dog', 53: 'pizza', 54: 'donut',\n",
        "      55: 'cake', 56: 'chair', 57: 'couch', 58: 'potted plant', 59: 'bed', 60: 'dining table', 61: 'toilet', 62: 'tv', 63: 'laptop', 64: 'mouse', 65: 'remote', 66: 'keyboard',\n",
        "      67: 'cell phone', 68: 'microwave', 69: 'oven', 70: 'toaster', 71: 'sink', 72: 'refrigerator', 73: 'book', 74: 'clock', 75: 'vase', 76: 'scissors', 77: 'teddy bear',\n",
        "      78: 'hair drier', 79: 'toothbrush'}\n",
        "\n",
        "img = cv2.imread('./bus.jpg')\n",
        "onnx_model = YOLOs(model_path=output_path.replace('.onnx', '_float16.onnx'))\n",
        "results = onnx_model.predict([img], conf=0.25, iou=0.7, agnostic=False, max_det=300)\n",
        "\n",
        "plt.imshow(plot(img.copy(), results[0].copy(), labels)[:, :, ::-1])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LVa9YwR7FrLb"
      },
      "source": [
        "## Export to TFLite\n",
        "Usage: [[None]]()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "akOrJ7s0HXOY"
      },
      "source": [
        "**Step1.** Export the model to TFLite format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GNLPPe-rHVD9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "cb30e722-710e-473c-e923-3a44e689ae9e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ultralytics 8.3.39 ðŸš€ Python-3.10.12 torch-2.5.1+cu121 CPU (Intel Xeon 2.20GHz)\n",
            "YOLOv8n summary (fused): 168 layers, 3,151,904 parameters, 0 gradients, 8.7 GFLOPs\n",
            "\n",
            "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'yolov8n.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 84, 8400) (6.2 MB)\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirements ['sng4onnx>=1.0.1', 'onnx_graphsurgeon>=0.3.26', 'onnx2tf>1.17.5,<=1.22.3', 'tflite_support'] not found, attempting AutoUpdate...\n",
            "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
            "Collecting sng4onnx>=1.0.1\n",
            "  Downloading sng4onnx-1.0.4-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting onnx_graphsurgeon>=0.3.26\n",
            "  Downloading onnx_graphsurgeon-0.5.2-py2.py3-none-any.whl.metadata (8.1 kB)\n",
            "Collecting onnx2tf<=1.22.3,>1.17.5\n",
            "  Downloading onnx2tf-1.22.3-py3-none-any.whl.metadata (136 kB)\n",
            "     â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 136.6/136.6 kB 8.1 MB/s eta 0:00:00\n",
            "Collecting tflite_support\n",
            "  Downloading tflite_support-0.4.4-cp310-cp310-manylinux2014_x86_64.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from onnx_graphsurgeon>=0.3.26) (1.26.4)\n",
            "Requirement already satisfied: onnx>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from onnx_graphsurgeon>=0.3.26) (1.17.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tflite_support) (1.4.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tflite_support) (24.3.25)\n",
            "Requirement already satisfied: protobuf<4,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from tflite_support) (3.20.2)\n",
            "Collecting sounddevice>=0.4.4 (from tflite_support)\n",
            "  Downloading sounddevice-0.5.1-py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting pybind11>=2.6.0 (from tflite_support)\n",
            "  Downloading pybind11-2.13.6-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.10/dist-packages (from sounddevice>=0.4.4->tflite_support) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from CFFI>=1.0->sounddevice>=0.4.4->tflite_support) (2.22)\n",
            "Downloading sng4onnx-1.0.4-py3-none-any.whl (5.9 kB)\n",
            "Downloading onnx_graphsurgeon-0.5.2-py2.py3-none-any.whl (56 kB)\n",
            "   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 56.4/56.4 kB 217.8 MB/s eta 0:00:00\n",
            "Downloading onnx2tf-1.22.3-py3-none-any.whl (435 kB)\n",
            "   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 435.0/435.0 kB 40.2 MB/s eta 0:00:00\n",
            "Downloading tflite_support-0.4.4-cp310-cp310-manylinux2014_x86_64.whl (60.8 MB)\n",
            "   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 60.8/60.8 MB 223.1 MB/s eta 0:00:00\n",
            "Downloading pybind11-2.13.6-py3-none-any.whl (243 kB)\n",
            "   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 243.3/243.3 kB 285.7 MB/s eta 0:00:00\n",
            "Downloading sounddevice-0.5.1-py3-none-any.whl (32 kB)\n",
            "Installing collected packages: sng4onnx, pybind11, onnx2tf, sounddevice, onnx_graphsurgeon, tflite_support\n",
            "Successfully installed onnx2tf-1.22.3 onnx_graphsurgeon-0.5.2 pybind11-2.13.6 sng4onnx-1.0.4 sounddevice-0.5.1 tflite_support-0.4.4\n",
            "\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m AutoUpdate success âœ… 7.3s, installed 4 packages: ['sng4onnx>=1.0.1', 'onnx_graphsurgeon>=0.3.26', 'onnx2tf>1.17.5,<=1.22.3', 'tflite_support']\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m âš ï¸ \u001b[1mRestart runtime or rerun command for updates to take effect\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[34m\u001b[1mTensorFlow SavedModel:\u001b[0m starting export with tensorflow 2.17.1...\n",
            "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/calibration_image_sample_data_20x128x128x3_float32.npy.zip to 'calibration_image_sample_data_20x128x128x3_float32.npy.zip'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.11M/1.11M [00:00<00:00, 26.7MB/s]\n",
            "Unzipping calibration_image_sample_data_20x128x128x3_float32.npy.zip to /content/calibration_image_sample_data_20x128x128x3_float32.npy...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 46.48file/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.17.0 opset 19...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mONNX:\u001b[0m slimming with onnxslim 0.1.42...\n",
            "\u001b[34m\u001b[1mONNX:\u001b[0m export success âœ… 1.2s, saved as 'yolov8n.onnx' (12.3 MB)\n",
            "\u001b[34m\u001b[1mTensorFlow SavedModel:\u001b[0m starting TFLite export with onnx2tf 1.22.3...\n",
            "\u001b[34m\u001b[1mTensorFlow SavedModel:\u001b[0m export success âœ… 24.0s, saved as 'yolov8n_saved_model' (30.9 MB)\n",
            "\n",
            "\u001b[34m\u001b[1mTensorFlow Lite:\u001b[0m starting export with tensorflow 2.17.1...\n",
            "\u001b[34m\u001b[1mTensorFlow Lite:\u001b[0m export success âœ… 0.0s, saved as 'yolov8n_saved_model/yolov8n_float32.tflite' (12.3 MB)\n",
            "\n",
            "Export complete (24.5s)\n",
            "Results saved to \u001b[1m/content\u001b[0m\n",
            "Predict:         yolo predict task=detect model=yolov8n_saved_model/yolov8n_float32.tflite imgsz=640  \n",
            "Validate:        yolo val task=detect model=yolov8n_saved_model/yolov8n_float32.tflite imgsz=640 data=coco.yaml  \n",
            "Visualize:       https://netron.app\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'yolov8n_saved_model/yolov8n_float32.tflite'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "output_path = model.export(format='tflite', imgsz=640)\n",
        "output_path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZU3t_juHX80"
      },
      "source": [
        "**Step2.** Validate the accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PvebGwiZHYET",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cde26c5d-c613-421a-d0dd-607dc6fe19ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING âš ï¸ Unable to automatically guess model task, assuming 'task=detect'. Explicitly define task for your model, i.e. 'task=detect', 'segment', 'classify','pose' or 'obb'.\n",
            "Ultralytics 8.3.39 ðŸš€ Python-3.10.12 torch-2.5.1+cu121 CPU (Intel Xeon 2.20GHz)\n",
            "Loading yolov8n_saved_model/yolov8n_float32.tflite for TensorFlow Lite inference...\n",
            "Setting batch=1 input of shape (1, 3, 640, 640)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/datasets/coco8/labels/val.cache... 4 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<?, ?it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00,  5.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          4         17      0.708      0.833      0.809      0.609\n",
            "                person          3         10      0.781        0.5      0.541      0.278\n",
            "                   dog          1          1      0.451          1      0.497      0.348\n",
            "                 horse          1          2      0.817          1      0.995      0.747\n",
            "              elephant          1          2      0.896        0.5      0.828      0.392\n",
            "              umbrella          1          1      0.635          1      0.995      0.995\n",
            "          potted plant          1          1      0.667          1      0.995      0.895\n",
            "Speed: 1.5ms preprocess, 171.6ms inference, 0.0ms loss, 2.2ms postprocess per image\n",
            "Results saved to \u001b[1mruns/detect/val3\u001b[0m\n",
            "0.6092337187926128\n",
            "0.808564761582811\n"
          ]
        }
      ],
      "source": [
        "delegated_model = YOLO(output_path)\n",
        "metrics = delegated_model.val(data=\"coco8.yaml\")\n",
        "\n",
        "print(metrics.box.map)  # mAP50-95\n",
        "print(metrics.box.map50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BWklgbxIISNf"
      },
      "outputs": [],
      "source": [
        "delegated_model = YOLO(output_path.replace('float32', 'float16'))\n",
        "metrics = delegated_model.val(data=\"coco8.yaml\")\n",
        "\n",
        "print(metrics.box.map)  # mAP50-95\n",
        "print(metrics.box.map50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EByhTWAYIl_c"
      },
      "source": [
        "**Step3.** Reproduce the output process manually."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QPPwRZQrVmIQ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class YOLOs():\n",
        "    def __init__(self, model_path, nc = 1):\n",
        "        self.interpreter = tf_lite.Interpreter(model_path=model_path)\n",
        "        self.input_details = self.interpreter.get_input_details()\n",
        "        self.output_details = self.interpreter.get_output_details()\n",
        "        self.interpreter.allocate_tensors()\n",
        "        self.X_axis = [0, 2]\n",
        "        self.y_axis = [1, 3]\n",
        "        self.nc = nc\n",
        "\n",
        "    def predict(self, frames, conf=0.25, iou=0.7, agnostic=False, max_det=300):\n",
        "        im = self.preprocess(frames)\n",
        "        preds = self.inference(im)\n",
        "        results = self.postprocess(preds, conf_thres=conf, iou_thres=iou, agnostic=agnostic, max_det=max_det)\n",
        "        return results\n",
        "\n",
        "    def postprocess(self, preds, conf_thres=0.25, iou_thres=0.45, classes=None, agnostic=False, multi_label=False, labels=(), max_det=300, nc=0, max_time_img=0.05, max_nms=30000, max_wh=7680, in_place=True, rotated=False):\n",
        "        xc = np.max(preds[:, 4: self.nc + 4], axis = 1) > conf_thres\n",
        "        preds = np.transpose(preds, (0, 2, 1))\n",
        "        preds[..., :4] = xywh2xyxy(preds[..., :4])\n",
        "        x = preds[0][xc[0]]\n",
        "\n",
        "        if not x.shape[0]:\n",
        "          return None\n",
        "        box, cls, keypoints = x[:, :4], x[:, 4:5], x[:, 5:]\n",
        "        j = np.argmax(cls, axis=1)\n",
        "        conf = cls[[i for i in range(len(j))], j]\n",
        "        concatenated = np.concatenate((box, conf.reshape(-1, 1), j.reshape(-1, 1).astype(float), keypoints), axis=1)\n",
        "        x = concatenated[conf.flatten() > conf_thres]\n",
        "\n",
        "        if x.shape[0] > max_nms:  # excess boxes\n",
        "            x = x[x[:, 4].argsort(descending=True)[:max_nms]]\n",
        "        cls = x[:, 5:6] * (0 if agnostic else max_wh)\n",
        "        scores, boxes = x[:, 4], x[:, :4] + cls\n",
        "\n",
        "        i = non_max_suppression(boxes, scores, iou_thres)\n",
        "        return [x[i[:max_det]]]\n",
        "\n",
        "    def inference(self, im):\n",
        "        self.interpreter.set_tensor(self.input_details[0]['index'], im)\n",
        "        self.interpreter.invoke()\n",
        "        preds = self.interpreter.get_tensor(self.output_details[0]['index'])\n",
        "        preds[:, self.X_axis] *= im.shape[1] ; preds[:, self.y_axis] *= im.shape[2]\n",
        "        return preds\n",
        "\n",
        "    def preprocess(self, im):\n",
        "        im = np.stack(self.pre_transform(im))\n",
        "        im = im[..., ::-1]\n",
        "        im = np.ascontiguousarray(im).astype(np.float32)\n",
        "        im /= 255.0\n",
        "        return im\n",
        "\n",
        "    def pre_transform(self, im):\n",
        "        imgsz = self.input_details[0]['shape'][1:3]\n",
        "        return [cv2.resize(im[0], imgsz, interpolation=cv2.INTER_LINEAR) for x in im]\n",
        "\n",
        "class LetterBox:\n",
        "    def __init__(self, new_shape=(640, 640), auto=False, scaleFill=False, scaleup=True, center=True, stride=32):\n",
        "        self.new_shape = new_shape\n",
        "        self.auto = auto\n",
        "        self.scaleFill = scaleFill\n",
        "        self.scaleup = scaleup\n",
        "        self.stride = stride\n",
        "        self.center = center\n",
        "\n",
        "    def __call__(self, labels=None, image=None):\n",
        "        if labels is None:\n",
        "            labels = {}\n",
        "        img = labels.get(\"img\") if image is None else image\n",
        "        shape = img.shape[:2]\n",
        "        new_shape = labels.pop(\"rect_shape\", self.new_shape)\n",
        "        if isinstance(new_shape, int):\n",
        "            new_shape = (new_shape, new_shape)\n",
        "\n",
        "        r = min(new_shape[0] / shape[0], new_shape[1] / shape[1])\n",
        "        if not self.scaleup:\n",
        "            r = min(r, 1.0)\n",
        "\n",
        "        ratio = r, r\n",
        "        new_unpad = int(round(shape[1] * r)), int(round(shape[0] * r))\n",
        "        dw, dh = new_shape[1] - new_unpad[0], new_shape[0] - new_unpad[1]  # wh padding\n",
        "        if self.auto:\n",
        "            dw, dh = np.mod(dw, self.stride), np.mod(dh, self.stride)  # wh padding\n",
        "        elif self.scaleFill:\n",
        "            dw, dh = 0.0, 0.0\n",
        "            new_unpad = (new_shape[1], new_shape[0])\n",
        "            ratio = new_shape[1] / shape[1], new_shape[0] / shape[0]  # width, height ratios\n",
        "\n",
        "        if self.center:\n",
        "            dw /= 2\n",
        "            dh /= 2\n",
        "\n",
        "        if shape[::-1] != new_unpad:\n",
        "            img = cv2.resize(img, new_unpad, interpolation=cv2.INTER_LINEAR)\n",
        "        top, bottom = int(round(dh - 0.1)) if self.center else 0, int(round(dh + 0.1))\n",
        "        left, right = int(round(dw - 0.1)) if self.center else 0, int(round(dw + 0.1))\n",
        "        img = cv2.copyMakeBorder(img, top, bottom, left, right, cv2.BORDER_CONSTANT, value=(114, 114, 114))\n",
        "        if labels.get(\"ratio_pad\"):\n",
        "            labels[\"ratio_pad\"] = (labels[\"ratio_pad\"], (left, top))  # for evaluation\n",
        "\n",
        "        if len(labels):\n",
        "            labels = self._update_labels(labels, ratio, dw, dh)\n",
        "            labels[\"img\"] = img\n",
        "            labels[\"resized_shape\"] = new_shape\n",
        "            return labels\n",
        "        else:\n",
        "            return img\n",
        "\n",
        "    def _update_labels(self, labels, ratio, padw, padh):\n",
        "        labels[\"instances\"].convert_bbox(format=\"xyxy\")\n",
        "        labels[\"instances\"].denormalize(*labels[\"img\"].shape[:2][::-1])\n",
        "        labels[\"instances\"].scale(*ratio)\n",
        "        labels[\"instances\"].add_padding(padw, padh)\n",
        "        return labels\n",
        "\n",
        "def plot(image, results, labels):\n",
        "    for bboxes in results:\n",
        "      x1, y1, x2, y2 = int(bboxes[0] * image.shape[1] / 640), int(bboxes[1] * image.shape[0] / 640), int(bboxes[2] * image.shape[1] / 640), int(bboxes[3] * image.shape[0] / 640)\n",
        "      conf, cls = bboxes[4] , bboxes[5]\n",
        "      cv2.rectangle(image, (x1, y1), (x2, y2), color=(0, 255, 0), thickness=3)\n",
        "      cv2.putText(image, f'{labels[int(cls)]} {conf:.2f}', (x1, y1 - 2), 0, 1, [0, 255, 0], thickness=2, lineType=cv2.LINE_AA)\n",
        "    return image\n",
        "\n",
        "def xywh2xyxy(x):\n",
        "    assert x.shape[-1] == 4, f\"input shape last dimension expected 4 but input shape is {x.shape}\"\n",
        "    y = np.empty_like(x)\n",
        "    xy = x[..., :2]\n",
        "    wh = x[..., 2:] / 2\n",
        "    y[..., :2] = xy - wh\n",
        "    y[..., 2:] = xy + wh\n",
        "    return y\n",
        "\n",
        "def non_max_suppression(boxes, scores, iou_threshold):\n",
        "    x1 = boxes[:, 0]\n",
        "    y1 = boxes[:, 1]\n",
        "    x2 = boxes[:, 2]\n",
        "    y2 = boxes[:, 3]\n",
        "\n",
        "    areas = (x2 - x1) * (y2 - y1)\n",
        "    order = scores.argsort()[::-1]\n",
        "\n",
        "    keep = []\n",
        "    while order.size > 0:\n",
        "        i = order[0]\n",
        "        keep.append(i)\n",
        "        xx1 = np.maximum(x1[i], x1[order[1:]])\n",
        "        yy1 = np.maximum(y1[i], y1[order[1:]])\n",
        "        xx2 = np.minimum(x2[i], x2[order[1:]])\n",
        "        yy2 = np.minimum(y2[i], y2[order[1:]])\n",
        "        w = np.maximum(0.0, xx2 - xx1)\n",
        "        h = np.maximum(0.0, yy2 - yy1)\n",
        "        inter = w * h\n",
        "        iou = inter / (areas[i] + areas[order[1:]] - inter)\n",
        "        inds = np.where(iou <= iou_threshold)[0]\n",
        "        order = order[inds + 1]\n",
        "\n",
        "    return np.array(keep)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IzvYF0QlVo14"
      },
      "outputs": [],
      "source": [
        "!wget https://ultralytics.com/images/bus.jpg -O bus.jpg\n",
        "import tensorflow.lite as tf_lite\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "\n",
        "labels = {0: 'person', 1: 'bicycle', 2: 'car', 3: 'motorcycle', 4: 'airplane', 5: 'bus', 6: 'train', 7: 'truck', 8: 'boat', 9: 'traffic light', 10: 'fire hydrant',\n",
        "      11: 'stop sign', 12: 'parking meter', 13: 'bench', 14: 'bird', 15: 'cat', 16: 'dog', 17: 'horse', 18: 'sheep', 19: 'cow', 20: 'elephant', 21: 'bear',\n",
        "      22: 'zebra', 23: 'giraffe', 24: 'backpack', 25: 'umbrella', 26: 'handbag', 27: 'tie', 28: 'suitcase', 29: 'frisbee', 30: 'skis', 31: 'snowboard', 32: 'sports ball',\n",
        "      33: 'kite', 34: 'baseball bat', 35: 'baseball glove', 36: 'skateboard', 37: 'surfboard', 38: 'tennis racket', 39: 'bottle', 40: 'wine glass', 41: 'cup', 42: 'fork',\n",
        "      43: 'knife', 44: 'spoon', 45: 'bowl', 46: 'banana', 47: 'apple', 48: 'sandwich', 49: 'orange', 50: 'broccoli', 51: 'carrot', 52: 'hot dog', 53: 'pizza', 54: 'donut',\n",
        "      55: 'cake', 56: 'chair', 57: 'couch', 58: 'potted plant', 59: 'bed', 60: 'dining table', 61: 'toilet', 62: 'tv', 63: 'laptop', 64: 'mouse', 65: 'remote', 66: 'keyboard',\n",
        "      67: 'cell phone', 68: 'microwave', 69: 'oven', 70: 'toaster', 71: 'sink', 72: 'refrigerator', 73: 'book', 74: 'clock', 75: 'vase', 76: 'scissors', 77: 'teddy bear',\n",
        "      78: 'hair drier', 79: 'toothbrush'}\n",
        "\n",
        "img = cv2.imread('./bus.jpg')\n",
        "tflite_model = YOLOs(model_path=output_path)\n",
        "results = tflite_model.predict([img], conf=0.25, iou=0.7, agnostic=False, max_det=300)\n",
        "\n",
        "plt.imshow(plot(img.copy(), results[0].copy(), labels)[:, :, ::-1])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Or428eYcSQ8p"
      },
      "source": [
        "## Export to TorchScript\n",
        "Usage: [[None]]()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "esTfPy7WSSys"
      },
      "source": [
        "**Step1.** Export the model to TorchScript format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2pU2T_0OSfVp"
      },
      "outputs": [],
      "source": [
        "output_path = model.export(format='torchscript')\n",
        "output_path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFTckSl-SzHN"
      },
      "source": [
        "**Step2.** Validate the accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zmZwjGSoSylk"
      },
      "outputs": [],
      "source": [
        "delegated_model = YOLO(output_path)\n",
        "metrics = delegated_model.val(data=\"coco8.yaml\")\n",
        "\n",
        "print(metrics.box.map)  # mAP50-95\n",
        "print(metrics.box.map50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HVUaHhKmSfma"
      },
      "source": [
        "**Step3.** Reproduce the output process manually."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RvSQwS8ESfho"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "class YOLOs():\n",
        "    def __init__(self, model_path, nc = 1, imgsz=(640, 640)):\n",
        "        self.torch_model = torch.jit.load(model_path)\n",
        "        self.X_axis = [0, 2]\n",
        "        self.y_axis = [1, 3]\n",
        "        self.nc = nc\n",
        "        self.imgsz = imgsz\n",
        "\n",
        "    def predict(self, frames, conf=0.25, iou=0.7, agnostic=False, max_det=300):\n",
        "        im = self.preprocess(frames)\n",
        "        preds = self.inference(im)\n",
        "\n",
        "        results = self.postprocess(preds, conf_thres=conf, iou_thres=iou, agnostic=agnostic, max_det=max_det)\n",
        "        return results\n",
        "\n",
        "    def postprocess(self, preds, conf_thres=0.25, iou_thres=0.45, classes=None, agnostic=False, multi_label=False, labels=(), max_det=300, nc=0, max_time_img=0.05, max_nms=30000, max_wh=7680, in_place=True, rotated=False):\n",
        "        xc = np.max(preds[:, 4: self.nc + 4], axis = 1) > conf_thres\n",
        "        preds = np.transpose(preds, (0, 2, 1))\n",
        "        preds[..., :4] = xywh2xyxy(preds[..., :4])\n",
        "        x = preds[0][xc[0]]\n",
        "\n",
        "        if not x.shape[0]:\n",
        "          return None\n",
        "        box, cls, keypoints = x[:, :4], x[:, 4:5], x[:, 5:]\n",
        "        j = np.argmax(cls, axis=1)\n",
        "        conf = cls[[i for i in range(len(j))], j]\n",
        "        concatenated = np.concatenate((box, conf.reshape(-1, 1), j.reshape(-1, 1).astype(float), keypoints), axis=1)\n",
        "        x = concatenated[conf.flatten() > conf_thres]\n",
        "\n",
        "        if x.shape[0] > max_nms:  # excess boxes\n",
        "            x = x[x[:, 4].argsort(descending=True)[:max_nms]]\n",
        "        cls = x[:, 5:6] * (0 if agnostic else max_wh)\n",
        "        scores, boxes = x[:, 4], x[:, :4] + cls\n",
        "\n",
        "        i = non_max_suppression(boxes, scores, iou_thres)\n",
        "        return [x[i[:max_det]]]\n",
        "\n",
        "    def inference(self, im):\n",
        "        with torch.no_grad():\n",
        "          im = torch.from_numpy(im)\n",
        "          preds = self.torch_model(im).numpy()\n",
        "        return preds\n",
        "\n",
        "    def preprocess(self, im):\n",
        "        im = np.stack(self.pre_transform(im))\n",
        "        im = im[..., ::-1]\n",
        "        im = np.ascontiguousarray(im).astype(np.float32)\n",
        "        im /= 255.0\n",
        "        im = np.transpose(im, (0, 3, 1, 2))\n",
        "        return im\n",
        "\n",
        "    def pre_transform(self, im):\n",
        "        return [cv2.resize(im[0], self.imgsz, interpolation=cv2.INTER_LINEAR) for x in im]\n",
        "\n",
        "class LetterBox:\n",
        "    def __init__(self, new_shape=(640, 640), auto=False, scaleFill=False, scaleup=True, center=True, stride=32):\n",
        "        self.new_shape = new_shape\n",
        "        self.auto = auto\n",
        "        self.scaleFill = scaleFill\n",
        "        self.scaleup = scaleup\n",
        "        self.stride = stride\n",
        "        self.center = center\n",
        "\n",
        "    def __call__(self, labels=None, image=None):\n",
        "        if labels is None:\n",
        "            labels = {}\n",
        "        img = labels.get(\"img\") if image is None else image\n",
        "        shape = img.shape[:2]\n",
        "        new_shape = labels.pop(\"rect_shape\", self.new_shape)\n",
        "        if isinstance(new_shape, int):\n",
        "            new_shape = (new_shape, new_shape)\n",
        "\n",
        "        r = min(new_shape[0] / shape[0], new_shape[1] / shape[1])\n",
        "        if not self.scaleup:\n",
        "            r = min(r, 1.0)\n",
        "\n",
        "        ratio = r, r\n",
        "        new_unpad = int(round(shape[1] * r)), int(round(shape[0] * r))\n",
        "        dw, dh = new_shape[1] - new_unpad[0], new_shape[0] - new_unpad[1]  # wh padding\n",
        "        if self.auto:\n",
        "            dw, dh = np.mod(dw, self.stride), np.mod(dh, self.stride)  # wh padding\n",
        "        elif self.scaleFill:\n",
        "            dw, dh = 0.0, 0.0\n",
        "            new_unpad = (new_shape[1], new_shape[0])\n",
        "            ratio = new_shape[1] / shape[1], new_shape[0] / shape[0]  # width, height ratios\n",
        "\n",
        "        if self.center:\n",
        "            dw /= 2\n",
        "            dh /= 2\n",
        "\n",
        "        if shape[::-1] != new_unpad:\n",
        "            img = cv2.resize(img, new_unpad, interpolation=cv2.INTER_LINEAR)\n",
        "        top, bottom = int(round(dh - 0.1)) if self.center else 0, int(round(dh + 0.1))\n",
        "        left, right = int(round(dw - 0.1)) if self.center else 0, int(round(dw + 0.1))\n",
        "        img = cv2.copyMakeBorder(img, top, bottom, left, right, cv2.BORDER_CONSTANT, value=(114, 114, 114))\n",
        "        if labels.get(\"ratio_pad\"):\n",
        "            labels[\"ratio_pad\"] = (labels[\"ratio_pad\"], (left, top))  # for evaluation\n",
        "\n",
        "        if len(labels):\n",
        "            labels = self._update_labels(labels, ratio, dw, dh)\n",
        "            labels[\"img\"] = img\n",
        "            labels[\"resized_shape\"] = new_shape\n",
        "            return labels\n",
        "        else:\n",
        "            return img\n",
        "\n",
        "    def _update_labels(self, labels, ratio, padw, padh):\n",
        "        labels[\"instances\"].convert_bbox(format=\"xyxy\")\n",
        "        labels[\"instances\"].denormalize(*labels[\"img\"].shape[:2][::-1])\n",
        "        labels[\"instances\"].scale(*ratio)\n",
        "        labels[\"instances\"].add_padding(padw, padh)\n",
        "        return labels\n",
        "\n",
        "def plot(image, results, labels):\n",
        "    for bboxes in results:\n",
        "      x1, y1, x2, y2 = int(bboxes[0] * image.shape[1] / 640), int(bboxes[1] * image.shape[0] / 640), int(bboxes[2] * image.shape[1] / 640), int(bboxes[3] * image.shape[0] / 640)\n",
        "      conf, cls = bboxes[4] , bboxes[5]\n",
        "      cv2.rectangle(image, (x1, y1), (x2, y2), color=(0, 255, 0), thickness=3)\n",
        "      cv2.putText(image, f'{labels[int(cls)]} {conf:.2f}', (x1, y1 - 2), 0, 1, [0, 255, 0], thickness=2, lineType=cv2.LINE_AA)\n",
        "    return image\n",
        "\n",
        "def xywh2xyxy(x):\n",
        "    assert x.shape[-1] == 4, f\"input shape last dimension expected 4 but input shape is {x.shape}\"\n",
        "    y = np.empty_like(x)\n",
        "    xy = x[..., :2]\n",
        "    wh = x[..., 2:] / 2\n",
        "    y[..., :2] = xy - wh\n",
        "    y[..., 2:] = xy + wh\n",
        "    return y\n",
        "\n",
        "def non_max_suppression(boxes, scores, iou_threshold):\n",
        "    x1 = boxes[:, 0]\n",
        "    y1 = boxes[:, 1]\n",
        "    x2 = boxes[:, 2]\n",
        "    y2 = boxes[:, 3]\n",
        "\n",
        "    areas = (x2 - x1) * (y2 - y1)\n",
        "    order = scores.argsort()[::-1]\n",
        "\n",
        "    keep = []\n",
        "    while order.size > 0:\n",
        "        i = order[0]\n",
        "        keep.append(i)\n",
        "        xx1 = np.maximum(x1[i], x1[order[1:]])\n",
        "        yy1 = np.maximum(y1[i], y1[order[1:]])\n",
        "        xx2 = np.minimum(x2[i], x2[order[1:]])\n",
        "        yy2 = np.minimum(y2[i], y2[order[1:]])\n",
        "        w = np.maximum(0.0, xx2 - xx1)\n",
        "        h = np.maximum(0.0, yy2 - yy1)\n",
        "        inter = w * h\n",
        "        iou = inter / (areas[i] + areas[order[1:]] - inter)\n",
        "        inds = np.where(iou <= iou_threshold)[0]\n",
        "        order = order[inds + 1]\n",
        "\n",
        "    return np.array(keep)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mGyNPhbjSfr8"
      },
      "outputs": [],
      "source": [
        "!wget https://ultralytics.com/images/bus.jpg -O bus.jpg\n",
        "import torch, cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "labels = {0: 'person', 1: 'bicycle', 2: 'car', 3: 'motorcycle', 4: 'airplane', 5: 'bus', 6: 'train', 7: 'truck', 8: 'boat', 9: 'traffic light', 10: 'fire hydrant',\n",
        "      11: 'stop sign', 12: 'parking meter', 13: 'bench', 14: 'bird', 15: 'cat', 16: 'dog', 17: 'horse', 18: 'sheep', 19: 'cow', 20: 'elephant', 21: 'bear',\n",
        "      22: 'zebra', 23: 'giraffe', 24: 'backpack', 25: 'umbrella', 26: 'handbag', 27: 'tie', 28: 'suitcase', 29: 'frisbee', 30: 'skis', 31: 'snowboard', 32: 'sports ball',\n",
        "      33: 'kite', 34: 'baseball bat', 35: 'baseball glove', 36: 'skateboard', 37: 'surfboard', 38: 'tennis racket', 39: 'bottle', 40: 'wine glass', 41: 'cup', 42: 'fork',\n",
        "      43: 'knife', 44: 'spoon', 45: 'bowl', 46: 'banana', 47: 'apple', 48: 'sandwich', 49: 'orange', 50: 'broccoli', 51: 'carrot', 52: 'hot dog', 53: 'pizza', 54: 'donut',\n",
        "      55: 'cake', 56: 'chair', 57: 'couch', 58: 'potted plant', 59: 'bed', 60: 'dining table', 61: 'toilet', 62: 'tv', 63: 'laptop', 64: 'mouse', 65: 'remote', 66: 'keyboard',\n",
        "      67: 'cell phone', 68: 'microwave', 69: 'oven', 70: 'toaster', 71: 'sink', 72: 'refrigerator', 73: 'book', 74: 'clock', 75: 'vase', 76: 'scissors', 77: 'teddy bear',\n",
        "      78: 'hair drier', 79: 'toothbrush'}\n",
        "\n",
        "img = cv2.imread('./bus.jpg')\n",
        "torch_model = YOLOs(model_path=output_path, imgsz=(640, 640))\n",
        "results = torch_model.predict([img], conf=0.25, iou=0.7, agnostic=False, max_det=300)\n",
        "plt.imshow(plot(img.copy(), results[0].copy(), labels)[:, :, ::-1])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qY227Lw3UDyo"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}